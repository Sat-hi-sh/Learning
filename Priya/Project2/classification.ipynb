{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 86528)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               22151424  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,244,929\n",
      "Trainable params: 22,244,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1516\\1514048432.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;31m# cbook must import matplotlib only within function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;31m# definitions, so it is safe to import from it here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatplotlibDeprecationWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msanitize_sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmplDeprecation\u001b[0m  \u001b[1;31m# deprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\rcsetup.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mls_mapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mColormap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_color_like\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfontconfig_pattern\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparse_fontconfig_pattern\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_enums\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mJoinStyle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCapStyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\colors.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumbers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNumber\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPngInfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;31m# Also note that Image.core is not a publicly documented interface,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;31m# and should be considered private and subject to change.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_imaging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"PILLOW_VERSION\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOTDIR=r\"C:\\Users\\prash\\OneDrive\\Desktop\\cancer detection 2\\DDSM Dataset\"\n",
    "no_of_images={}\n",
    "for dir in os.listdir(ROOTDIR):\n",
    "    no_of_images[dir]=len(os.listdir(os.path.join(ROOTDIR,dir)))\n",
    "no_of_images.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./train\"):\n",
    "    os.mkdir(\"./train\")\n",
    "    for dir in os.listdir(ROOTDIR):\n",
    "        os.mkdir(\"./train/\"+dir)\n",
    "        for img in np.random.choice(a=os.listdir(os.path.join(ROOTDIR,dir)),size=(math.floor(70/100*no_of_images[dir])-5),replace=False):\n",
    "            O=os.path.join(ROOTDIR,dir,img)\n",
    "            D=os.path.join(\"./train\",dir)\n",
    "            shutil.copy(O,D)\n",
    "            os.remove\n",
    "else:\n",
    "    print(\"Train Folder Exists\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./valid\"):\n",
    "    os.mkdir(\"./valid\")\n",
    "    for dir in os.listdir(ROOTDIR):\n",
    "        os.mkdir(\"./valid/\"+dir)\n",
    "        for img in np.random.choice(a=os.listdir(os.path.join(ROOTDIR,dir)),size=(math.floor(15/100*no_of_images[dir])-5),replace=False):\n",
    "            O=os.path.join(ROOTDIR,dir,img)\n",
    "            D=os.path.join(\"./valid\",dir)\n",
    "            shutil.copy(O,D)\n",
    "            os.remove\n",
    "else:\n",
    "    print(\"Valid Folder Exists\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./test\"):\n",
    "    os.mkdir(\"./test\")\n",
    "    for dir in os.listdir(ROOTDIR):\n",
    "        os.mkdir(\"./test/\"+dir)\n",
    "        for img in np.random.choice(a=os.listdir(os.path.join(ROOTDIR,dir)),size=(math.floor(15/100*no_of_images[dir])-5),replace=False):\n",
    "            O=os.path.join(ROOTDIR,dir,img)\n",
    "            D=os.path.join(\"./test\",dir)\n",
    "            shutil.copy(O,D)\n",
    "            os.remove\n",
    "else:\n",
    "    print(\"Test Folder Exists\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layer 1\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Convolutional Layer 2\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Convolutional Layer 3\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully Connected Layers\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification (cancer or non-cancer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9464 images belonging to 3 classes.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 111, 111, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 86528)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               22151424  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,244,929\n",
      "Trainable params: 22,244,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prash\\AppData\\Local\\Temp\\ipykernel_33504\\1361100249.py:70: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=train_generator, steps_per_epoch=50,epochs=40,verbose=1,validation_steps=16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "50/50 [==============================] - 175s 3s/step - loss: 0.6954 - accuracy: 0.5534\n",
      "Epoch 2/40\n",
      "50/50 [==============================] - 160s 3s/step - loss: 0.4889 - accuracy: 0.6954\n",
      "Epoch 3/40\n",
      "50/50 [==============================] - 194s 4s/step - loss: -1.0918 - accuracy: 0.6769\n",
      "Epoch 4/40\n",
      "50/50 [==============================] - 141s 3s/step - loss: -25.2986 - accuracy: 0.6069\n",
      "Epoch 5/40\n",
      "50/50 [==============================] - 176s 4s/step - loss: -438.0067 - accuracy: 0.6212\n",
      "Epoch 6/40\n",
      "50/50 [==============================] - 226s 5s/step - loss: -3874.7935 - accuracy: 0.5775\n",
      "Epoch 7/40\n",
      "50/50 [==============================] - 181s 4s/step - loss: -13394.7100 - accuracy: 0.5356\n",
      "Epoch 8/40\n",
      "50/50 [==============================] - 208s 4s/step - loss: -32768.3516 - accuracy: 0.5519\n",
      "Epoch 9/40\n",
      "50/50 [==============================] - 134s 3s/step - loss: -139444.0625 - accuracy: 0.5294\n",
      "Epoch 10/40\n",
      "50/50 [==============================] - 141s 3s/step - loss: -233305.8125 - accuracy: 0.5396\n",
      "Epoch 11/40\n",
      "50/50 [==============================] - 120s 2s/step - loss: -503700.6875 - accuracy: 0.5556\n",
      "Epoch 12/40\n",
      "50/50 [==============================] - 106s 2s/step - loss: -1045970.5625 - accuracy: 0.5462\n",
      "Epoch 13/40\n",
      "50/50 [==============================] - 107s 2s/step - loss: -1240241.0000 - accuracy: 0.5956\n",
      "Epoch 14/40\n",
      "50/50 [==============================] - 121s 2s/step - loss: -1617144.7500 - accuracy: 0.5775\n",
      "Epoch 15/40\n",
      "50/50 [==============================] - 102s 2s/step - loss: -3616882.2500 - accuracy: 0.5406\n",
      "Epoch 16/40\n",
      "50/50 [==============================] - 112s 2s/step - loss: -4702560.5000 - accuracy: 0.5981\n",
      "Epoch 17/40\n",
      "50/50 [==============================] - 108s 2s/step - loss: -7986450.0000 - accuracy: 0.5756\n",
      "Epoch 18/40\n",
      "50/50 [==============================] - 109s 2s/step - loss: -9477743.0000 - accuracy: 0.5456\n",
      "Epoch 19/40\n",
      "50/50 [==============================] - 104s 2s/step - loss: -13775135.0000 - accuracy: 0.5800\n",
      "Epoch 20/40\n",
      "50/50 [==============================] - 108s 2s/step - loss: -20094288.0000 - accuracy: 0.5625\n",
      "Epoch 21/40\n",
      "50/50 [==============================] - 103s 2s/step - loss: -39952032.0000 - accuracy: 0.5806\n",
      "Epoch 22/40\n",
      "50/50 [==============================] - 92s 2s/step - loss: -35151300.0000 - accuracy: 0.5810\n",
      "Epoch 23/40\n",
      "50/50 [==============================] - 103s 2s/step - loss: -50979340.0000 - accuracy: 0.6005\n",
      "Epoch 24/40\n",
      "50/50 [==============================] - 102s 2s/step - loss: -43406868.0000 - accuracy: 0.6019\n",
      "Epoch 25/40\n",
      "50/50 [==============================] - 109s 2s/step - loss: -54508088.0000 - accuracy: 0.6044\n",
      "Epoch 26/40\n",
      "50/50 [==============================] - 104s 2s/step - loss: -71763776.0000 - accuracy: 0.6031\n",
      "Epoch 27/40\n",
      "50/50 [==============================] - 95s 2s/step - loss: -111094976.0000 - accuracy: 0.5825\n",
      "Epoch 28/40\n",
      "50/50 [==============================] - 92s 2s/step - loss: -90550920.0000 - accuracy: 0.5981\n",
      "Epoch 29/40\n",
      "50/50 [==============================] - 100s 2s/step - loss: -247574176.0000 - accuracy: 0.5850\n",
      "Epoch 30/40\n",
      "50/50 [==============================] - 101s 2s/step - loss: -160232352.0000 - accuracy: 0.6081\n",
      "Epoch 31/40\n",
      "50/50 [==============================] - 99s 2s/step - loss: -201365696.0000 - accuracy: 0.6069\n",
      "Epoch 32/40\n",
      "50/50 [==============================] - 103s 2s/step - loss: -240288720.0000 - accuracy: 0.5950\n",
      "Epoch 33/40\n",
      "50/50 [==============================] - 101s 2s/step - loss: -171795056.0000 - accuracy: 0.6244\n",
      "Epoch 34/40\n",
      "50/50 [==============================] - 106s 2s/step - loss: -352847680.0000 - accuracy: 0.6112\n",
      "Epoch 35/40\n",
      "50/50 [==============================] - 102s 2s/step - loss: -284213376.0000 - accuracy: 0.6194\n",
      "Epoch 36/40\n",
      "50/50 [==============================] - 94s 2s/step - loss: -338844000.0000 - accuracy: 0.6331\n",
      "Epoch 37/40\n",
      "50/50 [==============================] - 103s 2s/step - loss: -493737536.0000 - accuracy: 0.6294\n",
      "Epoch 38/40\n",
      "50/50 [==============================] - 107s 2s/step - loss: -620799168.0000 - accuracy: 0.6331\n",
      "Epoch 39/40\n",
      "50/50 [==============================] - 111s 2s/step - loss: -799155584.0000 - accuracy: 0.6044\n",
      "Epoch 40/40\n",
      "50/50 [==============================] - 94s 2s/step - loss: -684353344.0000 - accuracy: 0.6256\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from keras.layers import Conv2D,MaxPool2D,Dropout,Flatten,Dense,BatchNormalization,GlobalAvgPool2D\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import load_img,img_to_array\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.mobilenet import MobileNet,preprocess_input\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load and preprocess the mammogram dataset\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,        # Normalize pixel values to [0, 1]\n",
    "                                   shear_range=0.2,      # Augmentation: shear range\n",
    "                                   zoom_range=0.2,       # Augmentation: zoom range\n",
    "                                   horizontal_flip=True) # Augmentation: horizontal flip\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    r\"D:\\cancer detection 2\\train\",  # Path to the training dataset directory\n",
    "    target_size=(224, 224),   # Resize images to (224, 224)\n",
    "    batch_size=32,            # Batch size\n",
    "    class_mode='binary')     # Binary classification (cancer or non-cancer)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layer 1\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Convolutional Layer 2\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Convolutional Layer 3\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten the feature maps\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully Connected Layers\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Binary classification (cancer or non-cancer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# ... (Define the architecture of the CNN model as mentioned in the previous example)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit_generator(generator=train_generator, steps_per_epoch=50,epochs=40,verbose=1,validation_steps=16)\n",
    "# Save the trained model\n",
    "model.save('trained_model.h5')  # Save the model weights and architecture to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9464 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255,        # Normalize pixel values to [0, 1]\n",
    "                                   shear_range=0.2,      # Augmentation: shear range\n",
    "                                   zoom_range=0.2,       # Augmentation: zoom range\n",
    "                                   horizontal_flip=True) # Augmentation: horizontal flip\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    r\"D:\\cancer detection 2\\train\",  # Path to the training dataset directory\n",
    "    target_size=(224, 224),   # Resize images to (224, 224)\n",
    "    batch_size=32,            # Batch size\n",
    "    class_mode='binary')  \n",
    "\n",
    "labels=train_generator.class_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Benign': 0, 'Malignant': 1, 'Normal': 2}\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2015 images belonging to 3 classes.\n",
      "63/63 [==============================] - 103s 2s/step\n",
      "Accuracy: 44.17%\n",
      "Precision: 19.51%\n",
      "Recall: 44.17%\n",
      "F1-score: 27.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prash\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def ppi(path):\n",
    "    \"\"\"\n",
    "    input: path\n",
    "    output:Pre Processed images\n",
    "    \"\"\"\n",
    "    image_data=ImageDataGenerator(zoom_range=0.2,shear_range=0.2,preprocessing_function= preprocess_input,horizontal_flip=True)\n",
    "    image=image_data.flow_from_directory(directory=path,target_size=(224,224),batch_size=32,class_mode='binary')\n",
    "    return image\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(r'trained_model.h5')\n",
    "\n",
    "# Load the evaluation dataset  \n",
    "path=r'D:\\cancer detection 2\\valid'\n",
    "eval_generator = ppi(path)\n",
    "y_eval = eval_generator.classes\n",
    "\n",
    "# Normalize the input data (if needed)\n",
    "# X_eval = (X_eval - mean) / std\n",
    "\n",
    "# Make predictions on the evaluation dataset\n",
    "y_pred = model.predict(eval_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "accuracy = accuracy_score(y_eval, y_pred_classes)\n",
    "precision = precision_score(y_eval, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_eval, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_eval, y_pred_classes, average='weighted')\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"Precision: {:.2f}%\".format(precision * 100))\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "print(\"F1-score: {:.2f}%\".format(f1 * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 67 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001F729B6D160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 907ms/step\n",
      "The test image is NORMAL\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load the trained model\n",
    "model = keras.models.load_model('trained_model.h5')\n",
    "\n",
    "# Load the test image\n",
    "img_path = r\"D:\\leena mam\\Dataset\\Beignin\\D1_A_1545_1.LEFT_CC (2).png\"\n",
    " # Replace with your image file path\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = x / 255.0  # Normalize pixel values to [0, 1]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# Predict the class of the test image\n",
    "prediction = model.predict(x)\n",
    "\n",
    "if prediction < 0.5:\n",
    "    print('The test image is NORMAL')\n",
    "else:\n",
    "    print('The test image is MALIGNANT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
